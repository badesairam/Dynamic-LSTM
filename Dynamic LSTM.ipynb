{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting if sequence is linear or not\n",
    "this is a tutorial on how to implement a Dynamcic LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating linear and random sequences\n",
    "class 0 == linear sequences [10,11,12...]\n",
    "class 1 == random sequences [10,20,1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "n_samples = 1500\n",
    "max_seq_len = 10\n",
    "min_seq_len = 5\n",
    "max_val = 200\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    len1=random.randint(min_seq_len,max_seq_len)\n",
    "    if(random.random() < .5) :\n",
    "        start = random.randint(0,max_val-len1)\n",
    "        s = [float(i) for i in range(start,start+len1)]\n",
    "#         s=np.asarray(s)\n",
    "        data.append(s)\n",
    "        labels.append([1,0])\n",
    "    else :\n",
    "        s = [float(random.randint(0,max_val)) for i in range(len1)]\n",
    "#         s = np.asarray(s)\n",
    "        data.append(s)\n",
    "        labels.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.0, 174.0, 160.0, 196.0, 109.0], [29.0, 115.0, 31.0, 79.0, 199.0, 185.0, 126.0], [38.0, 60.0, 117.0, 113.0, 145.0], [94.0, 131.0, 103.0, 157.0, 161.0, 118.0, 199.0, 128.0, 154.0], [111.0, 87.0, 160.0, 154.0, 39.0, 128.0, 48.0, 149.0]]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# data = np.asarray(data)\n",
    "labels = np.asarray(labels)\n",
    "print(data[5:10])\n",
    "print(labels[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-047ed65ff157>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 50\n",
    "display_step = 100\n",
    "\n",
    "num_hidden = 64\n",
    "num_classes = 2\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32,name='lr_rate')\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "seqlen = tf.placeholder(tf.int32,[None],name='sequence_length')\n",
    "\n",
    "sequence_maximum_length = tf.reduce_max(seqlen)\n",
    "\n",
    "x = tf.placeholder(\"float\",[None,None,1])\n",
    "y = tf.placeholder(\"float\",[None,num_classes])\n",
    "\n",
    "# cell = tf.contrib.rnn.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(rnn_size), keep_prob)\n",
    "\n",
    "# x = tf.unstack(x,sequence_maximum_length,1)\n",
    "\n",
    "\n",
    "stacked_cells = tf.contrib.rnn.MultiRNNCell(\n",
    "    [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(num_hidden), keep_prob) for _ in range(num_layers)])\n",
    "\n",
    "##outputs obtained here have max_length of batch but computation is done only until\n",
    "init_state = stacked_cells.zero_state(batch_size, tf.float32)\n",
    "outputs, last_states = tf.nn.dynamic_rnn(stacked_cells,x,initial_state=init_state,sequence_length=seqlen,dtype=tf.float32)\n",
    "\n",
    "\n",
    "#softmax layer for output\n",
    "\n",
    "W = tf.Variable(tf.random_normal([num_hidden,num_classes]))\n",
    "\n",
    "b = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "\n",
    "##We need to get outputs at which dynamic rnn computation is stopped\n",
    "\n",
    "outputs = tf.stack(outputs)\n",
    "\n",
    "# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n",
    "#so converting it to [batch,time,depth]\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "index = tf.range(0,batch_size) * sequence_maximum_length + (seqlen -1)\n",
    "\n",
    "#inputs to softmax layers [batch_size*num_hidden]\n",
    "outputs = tf.gather(tf.reshape(outputs,[-1,num_hidden]),index)\n",
    "\n",
    "#logits \n",
    "preds = tf.matmul(outputs,W)+b\n",
    "\n",
    "#Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(preds,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]\n",
      "  [10 11 12 13 14]]\n",
      "\n",
      " [[15 16 17 18 19]\n",
      "  [20 21 22 23 24]\n",
      "  [25 26 27 28 29]]]\n",
      "[2 3]\n",
      "[array([[ 0,  1,  2,  3,  4],\n",
      "       [ 5,  6,  7,  8,  9],\n",
      "       [10, 11, 12, 13, 14]]), array([[15, 16, 17, 18, 19],\n",
      "       [20, 21, 22, 23, 24],\n",
      "       [25, 26, 27, 28, 29]])]\n"
     ]
    }
   ],
   "source": [
    "o = np.arange(30).reshape((2,3,5))\n",
    "print(o)\n",
    "xx = tf.unstack(o)\n",
    "o = tf.stack(o)\n",
    "# o = tf.transpose(o,[1,0,2])\n",
    "seqlen = np.array([3,1])\n",
    "# print(seqlen-1)\n",
    "seq_max_len = 3\n",
    "index = seqlen - 1\n",
    "index = tf.range(0,2)*3 + (seqlen - 1 )\n",
    "o = tf.reshape(o,[-1,5])\n",
    "o = tf.gather(tf.reshape(o,[-1,5]),index)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(index))\n",
    "    print(sess.run(xx))\n",
    "#tf.transpose(o,[1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "# data = np.stack(data)\n",
    "data_train = data[:1300]\n",
    "# data_train = data_train[:, :, np.newaxis]\n",
    "label_train =labels[:1300]\n",
    "data_test = data[1300:]\n",
    "label_test = labels[:1300]\n",
    "# for src in data_train:\n",
    "#     print(src.shape[0])\n",
    "# print(data_train.shape)\n",
    "# print(label_train.shape[0])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(sources,labels,batch_size):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    # print(len(sources)//batch_size)\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        seq_length = []\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = labels[start_i:start_i + batch_size]\n",
    "        \n",
    "        for source in sources_batch:\n",
    "            seq_length.append(len(source))\n",
    "        \n",
    "        yield sources_batch,targets_batch,seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :: 1, batch :: 0 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-75321887f06c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msource_lengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch :: {}, batch :: {} '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msource_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msource_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mkeep_probability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msource_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msource_lengths\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "keep_probability=0.5\n",
    "learning_rate=0.01\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch_i in range(1,num_epochs+1):\n",
    "        for batch_i,(source_batch, labels_batch,source_lengths) in enumerate(get_batches(data_train,label_train,batch_size)):\n",
    "            print('epoch :: {}, batch :: {} '.format(epoch_i,batch_i))\n",
    "            sess.run(optimizer,feed_dict={x:source_batch,y:labels_batch,seqlen:source_lengths,learning_rate:learning_rate,keep_prob:keep_probability})\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                acc,loss = sess.run([accuracy,cost],feed_dict={x:source_batch,y:labels_batch,seqlen:source_lengths})\n",
    "                print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (tf)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
